{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 - Kernel Regularized Least Squares and Neural Networks\n",
    "\n",
    "In this lab activity we consider the extension of regularized least squares to non-linear problems using kernel functions and neural networks.\n",
    "\n",
    "# Part 1 - Kernel Methods\n",
    "\n",
    "A brief summary of the tasks:\n",
    " 1. Generate a simple non-linear data-set\n",
    " 2. Use **linear** RLS to try and learn with such dataset\n",
    " 3. Use a **feature transformation** for learning with non-linear data\n",
    " 4. Implement various kernel functions\n",
    " 5. Implement kernel RLS\n",
    " 6. Generate a more complex non-linear data-set\n",
    " 7. Use kernel RLS for learning on non-linear data, use cross-validation to find the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.linalg\n",
    "import scipy.spatial\n",
    "from scipy.interpolate import griddata\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_err(Ypred, Ytrue):\n",
    "    return np.mean((Ypred-Ytrue)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Learning with a simple non-linear dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate quadratic data\n",
    "\n",
    "In this lab we are going to use regression datasets where the target `Y` is **not a linear function** of the inputs `X`.\n",
    "\n",
    "As a first example, see the following function to generate quadratic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic_data_gen(n, w, sigma_noise):\n",
    "    X = np.random.uniform(-3, 3, size=(n, 1))\n",
    "    Xsq = X ** 2\n",
    "    noise = np.random.normal(0, sigma_noise, size=(n, 1))\n",
    "\n",
    "    # Here we can use scalar multiplication since in dimension 1\n",
    "    Y = Xsq * w + noise\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, y_tr = quadratic_data_gen(100, np.array([3]), 0.5)\n",
    "\n",
    "print(f\"Shape of x: {x_tr.shape}, shape of y: {y_tr.shape}\")\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x_tr, y_tr);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Linear RLS\n",
    "\n",
    "The implementation of linear regularized least-squares is given below in the functions `rls_train` and `rls_predict`. \n",
    "\n",
    "Remember: regularized least-squares has the following form\n",
    "\n",
    "$$(X_{tr}^\\top X_{tr} + \\lambda n I)w = X_{tr}^\\top Y_{tr}$$\n",
    "\n",
    "\n",
    "Tasks:\n",
    " - Use RLS to train a linear regressor for the quadratic data. Comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rls_train(x, y, reg_par):\n",
    "    cov = x.T @ x + reg_par * x.shape[0] * np.eye(x.shape[1])\n",
    "    rhs = x.T @ y\n",
    "    w = np.linalg.solve(cov, rhs)\n",
    "    return w\n",
    "\n",
    "def rls_predict(x, w):\n",
    "    return x @ w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, y_tr = quadratic_data_gen(100, np.array([3]), 0.5)\n",
    "\n",
    "...\n",
    "pred_tr = ...\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x_tr, y_tr, label=\"True\")\n",
    "ax.scatter(x_tr, pred_tr, label=\"Predicted\")\n",
    "ax.legend(loc=\"best\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Feature transform\n",
    "\n",
    "There is a simple way to use a linear algorithm for learning non-linear data: transforming the input data in such a way to convert the problem into a linear one.\n",
    "\n",
    "This is a simple fix in some cases, but becomes cumbersome if the datasets are non-linear in a complex way.\n",
    "\n",
    "Here we adopt this approach to train a RLS classifier with the quadratic dataset:\n",
    " 1. Generate the quadratic dataset\n",
    " 2. Transform the data so that it becomes a (n, 2) matrix containing the original input, and a transformed version of itself. Clearly the correct transformation depends on the underlying function (a quadratic function!).\n",
    " 3. Use the RLS algorithm on the new dataset\n",
    " 4. Plot **and comment** on the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, y_tr = quadratic_data_gen(...)\n",
    "\n",
    "# Instead of just the given xs build a feature matrix with the xs and their squares:\n",
    "trsf_x_tr = np.concatenate([...], axis=1)\n",
    "assert trsf_x_tr.shape == (x_tr.shape[0], 2), f\"Shape of x_tr is {x_tr.shape}. Expected ({x_tr.shape[0]}, 2)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now train a linear regression function on the new data!\n",
    "trsf_w_rls = rls_train(...)\n",
    "pred_tr = rls_predict(...)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x_tr, y_tr, label=\"True\")\n",
    "ax.scatter(x_tr, pred_tr, label=\"Predicted\")\n",
    "ax.legend(loc=\"best\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Kernel Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement 3 types of kernel\n",
    "\n",
    "The `kernel_matrix` function takes as input two arrays of data, and outputs the kernel matrix evaluated at every pair of points.\n",
    "\n",
    "You should implement it using the formulas seen in class for the following kernels:\n",
    " - linear kernel -- here the `param` argument can be ignored\n",
    " - polynomial kernel -- here the `param` argument is the exponent of the kernel\n",
    " - gaussian kernel -- here the `param` argument is the kernel length-scale ($\\sigma$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_distances(X1, X2):\n",
    "    \"\"\"Compute the matrix of pairwise squared-distances between all points in X1 and in X2.\n",
    "    \"\"\"\n",
    "    return scipy.spatial.distance.cdist(X1, X2, metric='seuclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_matrix(X1, X2, kernel_type, param):\n",
    "    # X1 : array of shape n x d\n",
    "    # X2 : array of shape m x d\n",
    "    if kernel_type == 'linear':\n",
    "        return ...\n",
    "    elif kernel_type == 'polynomial':\n",
    "        exponent = param\n",
    "        return ...\n",
    "    elif kernel_type == 'gaussian':\n",
    "        lengthscale = param\n",
    "        return ...\n",
    "    else:\n",
    "        raise ValueError(kernel_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Visualize the kernel (e.g. of the Gaussian type) for random data with different length-scales. What can you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = np.random.randn(100, 5)\n",
    "lengthscale = ...\n",
    "K = kernel_matrix(D, D, \"gaussian\", lengthscale)\n",
    "fig, ax = plt.subplots()\n",
    "ax.matshow(K)\n",
    "ax.set_title(\"Gaussian kernel with sigma=%f\" % (lengthscale))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Kernel RLS\n",
    "\n",
    "Remember that, given kernel $K = k(x_i, x_j)$ for $i=(1, \\dots, n)$ and $j=(1, \\dots, n)$, KRLS learns a weight-vector with the following formula\n",
    "\n",
    "$$(K + n \\lambda I)w = Y$$\n",
    "\n",
    "and then predictions on some new point $\\tilde{x}$ are given by\n",
    "\n",
    "$$f^{\\mathrm{KRLS}}(\\tilde{x}) = k(\\tilde{x}, X^{\\mathrm{train}}) w$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def krls_train(x, y, reg_par, kernel_type, kernel_par):\n",
    "    w = np.linalg.solve(...)\n",
    "    return w\n",
    "def krls_predict(x_ts, x_tr, w, kernel_type, kernel_par):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test the functions you have implemented on the quadratic dataset.\n",
    "\n",
    "**Tasks:**\n",
    " 1. use the linear kernel, can you fit the data?\n",
    " 2. use the polynomial kernel, test the effect of the kernel parameter on the results:\n",
    "    - describe what happens with a low/high exponent in terms of the bias-variance tradeoff.\n",
    " 3. use the polynomial kernel, but fix the kernel parameter. Test the effect of the regularization parameter\n",
    "    - describe what happens with a low/high regularizer in terms of the bias-variance tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, y_tr = quadratic_data_gen(100, np.array([3]), 2.0)\n",
    "\n",
    "w_krls = krls_train(x_tr, y_tr, reg_par=..., kernel_type=\"polynomial\", kernel_par=...)\n",
    "pred_tr = krls_predict(x_tr, x_tr, w_krls, kernel_type=\"polynomial\", kernel_par=...)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x_tr, y_tr, label=\"True\")\n",
    "ax.scatter(x_tr, pred_tr, label=\"Predicted\")\n",
    "ax.legend(loc=\"best\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a more complex non-linear dataset\n",
    "\n",
    "Define a function to generate a d-dimensional synthetic dataset where the targets `Y` depend non-linearly on the variables `X`.\n",
    "\n",
    "The parameters of the function are:\n",
    " - n : the number of samples\n",
    " - d : the dimension of the samples\n",
    " - low_d : the lower-bound for the uniformly distributed samples\n",
    " - high_d : the higher-bound for the uniformly distributed samples\n",
    " - sigma_noise : standard deviation of Gaussian noise added to the targets\n",
    " \n",
    "It should return:\n",
    " - X : 2D array of size n, d which fits in the desired bounds\n",
    " - Y : 2D array of size n, 1 which is a non-linear function of `X` (and a linear function of `w`)\n",
    " \n",
    "Examples of non-linear regression functions:\n",
    " - polynomial dependence of the Y on the X data\n",
    " - logarithmic dependence\n",
    " - more complex transforms (e.g. trig functions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonlinear_data_gen(n, d, low_d, high_d, sigma_noise):\n",
    "    X = np.random.uniform(low=low_d, high=high_d, size=(n, d))\n",
    "    assert X.shape == (n, d), \"Shape of X incorrect\"\n",
    "    \n",
    "    Y = ...\n",
    "    assert Y.shape == (n, 1), \"Shape of Y incorrect\"\n",
    "    \n",
    "    # Add noise\n",
    "    noise = np.random.normal(0, sigma_noise, size=(n, 1))\n",
    "    Y_noisy = ...\n",
    "    \n",
    "    return X, Y_noisy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KRLS and cross-validation\n",
    "Now we will use KRLS and k-fold CV to learn on the complex non-linear datasets we have generated.\n",
    "\n",
    "In this last part you will have to do the following:\n",
    " 1. Implement a function called `krls_kfoldcv` to select the best regularization parameter **and** kernel parameter with k-fold cross-validation. Your life will be easier if you use the provided `krls_kfold_valerr` function.\n",
    " 2. Generate a non-linear dataset, find the best hyperparameters (using e.g. the Gaussian kernel) with the training set, and then use them to make predictions on the test-set. Are you able to fit your data well?\n",
    " 3. Analyse how the amount of noise (see the `sigma_noise` parameter of `nonlinear_data_gen`) influences the best lambda as selected by cross-validation. In particular answer to the following question:\n",
    "     - How does the best lambda change as you increase/decrease the amount of noise in your dataset? Why? \n",
    "     \n",
    "    **Hint: keep the kernel parameter fixed for this third task, otherwise it might be very hard to see**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def krls_kfold_valerr(x_tr, y_tr, num_folds, reg_par, kernel_type, kernel_par):\n",
    "    \"\"\"\n",
    "    Compute the k-fold cross-validation error for one KRLS model (with speficied regularization, \n",
    "    kernel and kernel parameter).\n",
    "    \n",
    "    This function returns both the training errors and the validation errors \n",
    "    obtained from CV (as numpy arrays).\n",
    "    \"\"\"\n",
    "    if num_folds <= 1:\n",
    "        raise Exception(\"Please supply a number of folds > 1\")\n",
    "\n",
    "    n_tot = x_tr.shape[0]\n",
    "    n_val = int(n_tot // num_folds)\n",
    "    \n",
    "    tr_errs, val_errs = [], []\n",
    "    # `split_idx`: a list of arrays, each containing the validation indices for 1 fold\n",
    "    rand_idx = np.random.choice(n_tot, size=n_tot, replace=False)\n",
    "    split_idx = np.array_split(rand_idx, num_folds)\n",
    "    for fold in range(num_folds):\n",
    "        # Set the indices in boolean mask for all validation samples to `True`\n",
    "        val_mask = np.zeros(n_tot, dtype=bool)\n",
    "        val_mask[split_idx[fold]] = True\n",
    "        \n",
    "        kf_x_tr = x_tr[~val_mask]\n",
    "        kf_y_tr = y_tr[~val_mask]\n",
    "        kf_x_val = x_tr[val_mask]\n",
    "        kf_y_val = y_tr[val_mask]\n",
    "        \n",
    "        w_krls = krls_train(kf_x_tr, kf_y_tr, reg_par=reg_par, kernel_type=kernel_type, kernel_par=kernel_par)\n",
    "        \n",
    "        pred_tr = krls_predict(kf_x_tr, kf_x_tr, w_krls, kernel_type=kernel_type, kernel_par=kernel_par)\n",
    "        pred_val = krls_predict(kf_x_val, kf_x_tr, w_krls, kernel_type=kernel_type, kernel_par=kernel_par)\n",
    "        tr_errs.append(calc_err(pred_tr, kf_y_tr))\n",
    "        val_errs.append(calc_err(pred_val, kf_y_val))\n",
    "    return np.asarray(tr_errs), np.asarray(val_errs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def krls_kfoldcv(x_tr, y_tr, num_folds, reg_par_list, kernel_type, kernel_par_list):\n",
    "    \"\"\"Choose the best parameters for both the regularizer and the kernel parameter according to K-Fold CV.\n",
    "    \"\"\"\n",
    "    for i, reg_par in enumerate(reg_par_list):\n",
    "        for j, kernel_par in enumerate(kernel_par_list):\n",
    "            ...\n",
    "    best_reg_par = ...\n",
    "    best_kernel_par = ...\n",
    "    best_err = ...\n",
    "    print(f\"The best error (MSE={best_err}) was obtained with \"\n",
    "          f\"lambda={best_reg_par}, kernel-parameter={best_kernel_par}\")\n",
    "    return best_reg_par, best_kernel_par, best_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, y_tr = nonlinear_data_gen(...)\n",
    "x_ts, y_ts = nonlinear_data_gen(...)\n",
    "krls_kfoldcv(...)\n",
    "\n",
    "# Now retrain on the whole of x_tr with the best parameters and test on x_ts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyse the effect of noise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Neural Network\n",
    "## Instructions on using Keras\n",
    "\n",
    "Models in Keras are defined as a sequence of layers.\n",
    "\n",
    "We create a [Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) object which holds multiple layers executed one by one, and add layers to the object until we have formed the architecture.\n",
    "\n",
    "To create the first layer, you need to know the right number of input features. You can specify this by using the `input_dim` argument. \n",
    "\n",
    "The things to choose when defining the architecture are many:\n",
    " - number of layers\n",
    " - type of layers\n",
    " - size of layers\n",
    " - type of non-linearity\n",
    " - whether or not to add regularization\n",
    " \n",
    "Here we will use only fully-connected (dense) layers, so the type of layer is fixed. Fully connected layers are defined using the [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) class, which takes as parameters the number of neurons (which is the **dimension of the output**).\n",
    "\n",
    "The activation functions are used after each dense layer. You can choose the activation functions for hidden layers yourself, a common choice being the [ReLU](https://www.tensorflow.org/api_docs/python/tf/keras/activations/relu) activation. But for the last layer, the activation must reflect the range of the outputs.\n",
    "\n",
    "Since we will work with binary classification problem, the output should be between 0 and 1, which is then easy to map to any given class. To ensure this we can use the [Sigmoid](https://www.tensorflow.org/api_docs/python/tf/keras/activations/sigmoid) activation.\n",
    "\n",
    "\n",
    "After having created a model you need to **compile** it. During the compilation phase you must specify some parameters related to how the model will be optimized:\n",
    " - The `optimizer`. For the following exercise you should use [SGD](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD), initialized with some learning rate (instructions on how to choose it follow).\n",
    " - The `loss` function. For binary classification you can use the [cross-entropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy) loss.\n",
    " - A list of `metrics`: common error functions which you want keras to report at each training epoch.\n",
    " \n",
    "Then you may actually train the model by calling **fit**. The fit function takes as input the training data, and some more parameters related to the training process:\n",
    " - `epochs` : the number of epochs to train for\n",
    " - `batch_size` : the size of mini-batches. A high batch-size will speed up computations but may make training unstable.\n",
    " \n",
    "Other useful functions are `model.predict` which runs the model's forward pass to predict on new samples, and `model.evaluate` which is similar to `predict` but instead of giving predictions as output, it simply computes some metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_2d_cls_data(n, sigma_noise):\n",
    "    n_cls = n // 2\n",
    "    \n",
    "    theta = np.random.randn(n_cls) * 2 * np.pi\n",
    "    cls1 = np.stack([np.cos(theta) * 2, np.sin(theta) * 2], axis=1)\n",
    "    cls2 = np.random.randn(n_cls, 2) * 0.5\n",
    "\n",
    "    cls1 += np.random.randn(cls1.shape[0], 2) * sigma_noise * 3\n",
    "    cls2 += np.random.randn(cls2.shape[0], 2) * sigma_noise\n",
    "    \n",
    "    X = np.concatenate([cls1, cls2], axis=0)\n",
    "    y = np.concatenate([np.zeros(n_cls), np.ones(n_cls)], axis=0)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separatingFLR(data, labels, predictions, model):\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    num_grid_points = 100\n",
    "    xi = np.linspace(data[:, 0].min(), data[:, 0].max(), num_grid_points)\n",
    "    yi = np.linspace(data[:, 1].min(), data[:, 1].max(), num_grid_points)\n",
    "    gdata = np.stack([xi, yi], 1)\n",
    "    X, Y = np.meshgrid(xi,yi)\n",
    "    pred_grid = model.predict(\n",
    "        np.stack([X.reshape(-1), Y.reshape(-1)], axis=1)\n",
    "    ).reshape(num_grid_points, num_grid_points)\n",
    "    \n",
    "    ax.contour(xi, yi, pred_grid, 15, linewidths=2, colors='k', levels=[0.5])\n",
    "    # plot data points.\n",
    "    ax.scatter(data[:,0], data[:,1], c=labels.ravel(), marker='o', s=100, zorder=10, alpha=0.8)\n",
    "    ax.set_xlim(data[:,0].min(), data[:,0].max())\n",
    "    ax.set_ylim(data[:,1].min(), data[:,1].max())\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_err(predicted, observed):\n",
    "    predicted = predicted.ravel()\n",
    "    observed = observed.ravel()\n",
    "    threshold_preds = predicted.copy()\n",
    "    threshold_preds[predicted < 0.5] = 0\n",
    "    threshold_preds[predicted >= 0.5] = 1\n",
    "    return np.mean(threshold_preds != observed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Different Architectures\n",
    "\n",
    "Using a simple binary classification dataset you will build a keras model with Dense layers, and the RELU activation function.\n",
    "\n",
    "You will explore different architectures to try and see when the NN overfits or underfits the data.\n",
    "\n",
    "In particular, you should try the following:\n",
    " 1. A NN with a single hidden layer with few (e.g. 3, 4) neurons\n",
    " 2. A NN with a single hidden layer with many (e.g. 100) neurons\n",
    " 3. A NN with many (e.g. 3, 4, 5) hidden layers with a few neurons (e.g. 10 to 30).\n",
    " \n",
    "Train the neural network using the SGD algorithm with a learning rate of 0.05 (you may explore different values) for 500 epochs (or less if time doesn't permit).\n",
    "\n",
    "For each setting you try, plot the training and validation errors as a function of the epochs, and plot the separating function (use the `separatingFLR` function).\n",
    "\n",
    "For each setting comment on whether the NN is overfitting or not. Further comment on which model you believe is better, and why (e.g. computational or accuracy considerations).\n",
    "\n",
    "We provide some skeleton code to train and evaluate a model, you'll have to fill it in, and do the same thing multiple times for different models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating the train and test sets\n",
    "\n",
    "Note that with neural nets, the labels for binary classification should be 0 and +1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(34)\n",
    "Xtr, Ytr = generate_2d_cls_data(150, 0.2)\n",
    "Xte, Yte = generate_2d_cls_data(100, 0.2)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(Xtr[Ytr == 1,0], Xtr[Ytr == 1, 1])\n",
    "ax.scatter(Xtr[Ytr == 0,0], Xtr[Ytr == 0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a model and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build your first model by creating a Sequential object and then adding 3 Dense layers:\n",
    "model = Sequential()\n",
    "model.add(Dense(..., input_dim=..., activation='relu'))\n",
    "model.add(Dense(...))  # This is the last layer, it should have 1 neuron and the sigmoid activation\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.05), # Algorithm used for optimization\n",
    "    loss='binary_crossentropy',                         # The loss function\n",
    "    metrics=['accuracy'],                               # Metrics to evaluate the goodness of predictions\n",
    ")\n",
    "history = model.fit(\n",
    "    Xtr, Ytr,                       # Training data\n",
    "    epochs=600,                     # Number of training epochs\n",
    "    batch_size=10,                  # Train using mini-batches of 10 samples each\n",
    "    validation_split=0.2,           # Split the data using 80% to train and 20% for validation\n",
    ")\n",
    "# Predict and calculate errors\n",
    "train_preds = ...\n",
    "test_preds = ...\n",
    "train_err = ...\n",
    "test_err = ...\n",
    "print(\"Training error: %.2f%%, Test error: %.2f%%\" % (train_err * 100, test_err * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the results from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With this function you can plot the history of the model training produced by the fit function\n",
    "def plot_history(history):\n",
    "    fig, ax = plt.subplots()\n",
    "    # Plot training & validation accuracy values\n",
    "    ax.plot(history.history['accuracy'], label='Train')\n",
    "    ax.plot(history.history['val_accuracy'], label='Val')\n",
    "    ax.set_title('Model accuracy')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.legend(loc='best')\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = separatingFLR(Xtr, Ytr, train_preds, model)\n",
    "ax.set_title(\"Put the model name here!\");\n",
    "fig, ax = plot_history(history)\n",
    "ax.set_title(\"Put the model name here!\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Neural Network on MNIST!\n",
    "\n",
    "We can finally switch to a real dataset now.\n",
    "\n",
    "First we will load the MNIST dataset and plot it to see how it really looks like.\n",
    "\n",
    "Then we will use the notions learned in the first part to train a model which can distinguish two digits in the MNIST data.\n",
    "\n",
    "As a bonus, some of you may want to look into multi-class classification and train a model on the whole MNIST dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "print(\"Training set shapes: \", x_train.shape, y_train.shape)\n",
    "print(\"Test set shapes: \", x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the data, try plotting the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.imshow(x_train[1])\n",
    "ax.set_title(y_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a look at the targets. This is a multiclass problem!\n",
    "print(y_train[0], y_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bring the data into shape\n",
    "\n",
    "1. Choose the digits we want to classify (variables `num_1` and `num_2`)\n",
    "2. Then we restrict training and test sets to only use those numbers\n",
    "3. We reshape the images from 28*28 to a single 784-dimensional vector\n",
    "4. Finally we modify the labels to their appropriate range (0 and +1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_1 = 1\n",
    "num_2 = 7\n",
    "\n",
    "# Here we take only two digits from MNIST. \n",
    "# We will reduce the problem to binary classification.\n",
    "Xtr = x_train[(y_train == num_1) | (y_train == num_2)]\n",
    "Ytr = y_train[(y_train == num_1) | (y_train == num_2)]\n",
    "Xts = x_test[(y_test == num_1) | (y_test == num_2)]\n",
    "Yts = y_test[(y_test == num_1) | (y_test == num_2)]\n",
    "\n",
    "# Reshape the data correctly\n",
    "Xtr = Xtr.reshape(-1, 28*28)\n",
    "Xts = Xts.reshape(-1, 28*28)\n",
    "Ytr[Ytr == num_1] = 0\n",
    "Ytr[Ytr == num_2] = 1\n",
    "Yts[Yts == num_1] = 0\n",
    "Yts[Yts == num_2] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.imshow(Xtr[3].reshape(28, 28))\n",
    "ax.set_title(Ytr[3]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define The Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile The Keras Model\n",
    "\n",
    "Here we will use the Adam optimizer instead. It tends to work better than SGD with high dimensional data (such as our MNIST images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='binary_crossentropy', \n",
    "    optimizer=tf.keras.optimizers.Adam(...), \n",
    "    metrics=['accuracy'],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the keras model on the dataset\n",
    "history = model.fit(Xtr, Ytr, \n",
    "                    epochs=..., \n",
    "                    batch_size=...,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and calculate errors\n",
    "train_preds = ...\n",
    "test_preds = ...\n",
    "train_err = ...\n",
    "test_err = ...\n",
    "print(\"Training error: %.2f%%, Test error: %.2f%%\" % (train_err * 100, test_err * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the keras model. Is this consistent with the error you computed above?\n",
    "_, accuracy = model.evaluate(Xts, Yts)\n",
    "print('Test accuracy: %.2f%%' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, ax = plot_history(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
