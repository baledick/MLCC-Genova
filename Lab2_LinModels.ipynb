{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Regularized Least Square and Logistic Regression\n",
    "In this lab, we focus on RLS to address linear regression problems. \n",
    "\n",
    "In this lab, we have to:\n",
    "- **(Task 1)** implement RLS to solve linear regression problems\n",
    "- **(Task 2)** observe performance of RLS changing the noise in the data and the regularization parameter\n",
    "- **(Task 3)** implement K-Fold Cross-Validation algorithm for RLS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import scipy.linalg\n",
    "from scipy.interpolate import griddata\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation\n",
    "To generate linear regression data, we use the `linearRegrFunction` introduced in Lab1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearRegrFunction(n, D, low_D, high_D, W, sigma_noise):\n",
    "    X = np.zeros((n,D))\n",
    "    for i in range(0, D):\n",
    "        X[:,i] = np.random.uniform(low_D[i], high_D[i], size=n)\n",
    "    \n",
    "    gauss_noise = np.random.normal(0, sigma_noise, size=(n,1))\n",
    "\n",
    "    Y = np.dot(X, W) + gauss_noise\n",
    "    \n",
    "    return X, Y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Noiseless dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7feea0de09e8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 10\n",
    "D = 1\n",
    "low_D, high_D = [-1], [1]\n",
    "w = np.array(1.0).reshape(1, 1)\n",
    "noise_std = 0.0\n",
    "\n",
    "# Data generation\n",
    "X, Y = linearRegrFunction(n, D, low_D, high_D, w, noise_std)\n",
    "# Plot of the data\n",
    "_, ax = plt.subplots()\n",
    "ax.set_title(\"Linear Regression data without noise\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.scatter(X, Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RLS regressor\n",
    "We want to implement the `regularizedLSTrain` function which train RLS regression.\n",
    "\n",
    "The signature of `regularizedLSTrain` is the following:\n",
    "\n",
    "`w = regularizedLSTrain(Xtr, Ytr, lam)`\n",
    "\n",
    "where:\n",
    "- **Xtr** are the training inputs\n",
    "- **Ytr** are the training outputs\n",
    "- **lam** is the regularization parameter $\\lambda$\n",
    "\n",
    "To implement this function, you will need to use the following functions from numpy:\n",
    "\n",
    "- [`np.linalg.cholesky`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.cholesky.html)\n",
    "- [`scipy.linalg.solve_triangular`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.solve_triangular.html)\n",
    "\n",
    "Consider \n",
    "\n",
    "**$(X_{tr}^\\intercal X_{tr} + \\lambda n I)w = X_{tr}^\\intercal Y_{tr}$**\n",
    "\n",
    "Let $A = X_{tr}^\\intercal X_{tr} + \\lambda n I$ and $b = X_{tr}^\\intercal Y_{tr}$, we can find $w$ with the following steps:\n",
    "1. First build the left-hand side matrix `A`, and the right-hand side matrix `b`.\n",
    "2. Compute the Cholesky decomposition of `A` (note that the numpy function will provide a lower-triangular matrix)\n",
    "3. You will have to solve two triangular systems, one using the Cholesky decomposition, and the other using its transpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularizedLSTrain(Xtr, Ytr, lam):\n",
    "    # Insert your code here\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need also to implement a function `regularizedLSTest` which given a test set `Xte` and the `w` obtained using `regularizedLSTrain`, it returns `Ypred` containing the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularizedLSTest(w, Xte):\n",
    "    # Insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the performance of RLS regressor, we need a function to estimate the error.\n",
    "\n",
    "Given two vectors `Ytrue` (real outputs) and `Ypred` (predicted outputs), we can measure the error obtained when predicting `Ypred` instead of `Ytrue` with the MSE (Mean Square Error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcError(Ypred, Ytrue):\n",
    "    return np.mean((Ypred-Ytrue)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build an easy example to observe how our model works:\n",
    "- Generate a training set with **ntrain** points and a test set with **ntest** points \n",
    "- Train RLS with `regularizedLSTrain` function and test it with `regularizedLSTest` on test set\n",
    "- Compute the training and test error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain = 100\n",
    "ntest = 50\n",
    "D = 1\n",
    "low_D = [-1] * D\n",
    "high_D = [5] * D\n",
    "rnd_state = np.random.RandomState(42)\n",
    "w = rnd_state.randn(D, 1) \n",
    "noise_std = 0.1\n",
    "\n",
    "lam = 1e-3\n",
    "\n",
    "# Generate a training set with ntrain points and a test set with ntest \n",
    "Xtr, Ytr = ...\n",
    "Xte, Yte = ...\n",
    "\n",
    "# Train RLS\n",
    "\n",
    "\n",
    "# Compute predictions on training and test set\n",
    "Ytr_pred = ...\n",
    "Yte_pred = ...\n",
    "\n",
    "train_err = calcError(Ytr_pred, Ytr)\n",
    "test_err = calcError(Yte_pred, Yte)\n",
    "\n",
    "print(\"[--] Training error: {}\\tTest error: {}\".format(train_err, test_err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing $\\lambda$\n",
    "Now we can play with our model changing the noise level in the data and changing the $\\lambda$ parameter.\n",
    "\n",
    "Let's start by changing $\\lambda$ and fixing the other parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "D = 5\n",
    "low_D = [-5] * D\n",
    "high_D = [5] * D\n",
    "w = np.array([i for i in range(D)]).reshape(D, 1)\n",
    "noise_std = 0.1\n",
    "\n",
    "\n",
    "# Data generation\n",
    "Xtr, Ytr = ...\n",
    "Xte, Yte = ...\n",
    "\n",
    "lam_list = np.logspace(1e-8, 1.0, 50)\n",
    "tr_err = []\n",
    "te_err = []\n",
    "\n",
    "for lam in lam_list:\n",
    "\n",
    "    # Train RLS\n",
    "\n",
    "\n",
    "    # Compute predictions on training and test set\n",
    "\n",
    "    # Compute training and test error and store them on tr_err and te_err\n",
    "\n",
    "# Plot training and test error\n",
    "_, ax = plt.subplots()\n",
    "ax.set_title(\"Training/Test error\")\n",
    "ax.plot(lam_list, tr_err, '-', c=\"blue\", label=\"training error\")\n",
    "ax.plot(lam_list, te_err, '-', c=\"orange\", label=\"test error\")\n",
    "ax.legend(loc=\"best\")\n",
    "ax.set_xlabel(\"$\\lambda$\")\n",
    "ax.set_ylabel(\"MSE\")\n",
    "ax.set_xscale(\"log\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain here what happens when $\\lambda$ increases: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Cross Validation for RLS\n",
    "Now, we want to implement the K-Fold Cross Validation for RLS. \n",
    "\n",
    "In specific we want to implement the `KFoldCVRLS` function which, given a training set **Xtr** and **Ytr**, a number of folds **KF** and a set of values for $\\lambda$ (**regpar_list**) and returns the $\\lambda$ which minimize the average validation error **bestlam**, the mean validation error **val_mean**, the validation error variance **val_var**, the mean training error **tr_mean** and the training error variance **tr_var**.\n",
    "\n",
    "`bestlam, val_mean, val_var, tr_mean, tr_var = KFoldCVRLS(Xtr, Ytr, KF, regpar_list)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KFoldCVRLS(Xtr, Ytr, KF, regpar_list):\n",
    "    if KF <= 1:\n",
    "        raise Exception(\"Please supply a number of fold > 1\")\n",
    "\n",
    "    # Ensures that regpar_list is a numpy array\n",
    "    regpar_list = np.array(regpar_list)\n",
    "    num_regpar = regpar_list.size\n",
    "\n",
    "    n_tot = Xtr.shape[0]\n",
    "    n_val = int(n_tot // KF)\n",
    "\n",
    "    # We want to compute 1 error for each `k` and each fold\n",
    "    tr_errors = np.zeros((num_regpar, KF))\n",
    "    val_errors = np.zeros((num_regpar, KF))\n",
    "\n",
    "    for idx, regpar in enumerate(regpar_list):\n",
    "        # `split_idx`: a list of arrays, each containing the validation indices for 1 fold\n",
    "        rand_idx = np.random.choice(n_tot, size=n_tot, replace=False)\n",
    "        split_idx = np.array_split(rand_idx, KF)\n",
    "        for fold in range(KF):\n",
    "            # Set the indices in boolean mask for all validation samples to `True`\n",
    "            val_mask = np.zeros(n_tot, dtype=bool)\n",
    "            val_mask[split_idx[fold]] = True\n",
    "\n",
    "            # Use the boolean mask to split X, Y in training and validation part\n",
    "\n",
    "            X = ... # training input \n",
    "            Y = ... # training output \n",
    "            X_val = ... # validation input\n",
    "            Y_val = ... # validation output\n",
    "            \n",
    "            # Train a RLS model for a single fold, and the given value of `regpar`\n",
    "            currW = ... \n",
    "            \n",
    "            # Compute the training error of the RLS regression for the given value of regpar\n",
    "            YpredTR = regularizedLSTest(currW, X)\n",
    "            tr_errors[idx, fold] = calcError(YpredTR, Y)\n",
    "\n",
    "            # Compute the validation error of the RLS regression for the given value of regpar\n",
    "            YpredVAL = regularizedLSTest(currW, X_val)\n",
    "            val_errors[idx, fold] = calcError(YpredVAL, Y_val)\n",
    "            \n",
    "    # Calculate error statistics along the repetitions\n",
    "    tr_mean = np.mean(tr_errors, axis=1)\n",
    "    tr_var = np.var(tr_errors, axis=1)\n",
    "    val_mean = np.mean(val_errors, axis=1)\n",
    "    val_var = np.var(val_errors, axis=1)\n",
    "    \n",
    "    bestlam_idx = np.argmin(val_mean)\n",
    "    bestlam = regpar_list[bestlam_idx]\n",
    "\n",
    "    return bestlam, val_mean, val_var, tr_mean, tr_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `KFoldCVRLS` to find the best regularization parameter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "D = 1\n",
    "sigma_noise = 0.5\n",
    "truew = np.random.randn(D, 1)\n",
    "reg_pars = np.logspace(-5, 1, 100)\n",
    "KF = 5\n",
    "\n",
    "low_D = [-3] * D\n",
    "high_D = [3] * D\n",
    "\n",
    "# Generate training set\n",
    "Xtr, Ytr = ... \n",
    "\n",
    "# Compute best lambda\n",
    "bestlam, Vm, Vs, Tm, Ts = ...\n",
    "\n",
    "\n",
    "# Plot training and validation error\n",
    "_, ax = plt.subplots()\n",
    "ax.plot(reg_pars, Vm, '-o', label=\"Validation error\")\n",
    "ax.plot(reg_pars, Tm, '-o', label=\"Train error\")\n",
    "ax.axvline(bestlam, linestyle=\"--\", c=\"red\", alpha=0.7, label=\"best $\\lambda$\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlabel(\"$\\lambda$\")\n",
    "ax.set_ylabel(\"MSE\")\n",
    "ax.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the regression curve using the best $\\lambda$ (found with `KFoldCVRLS`) and using the worsk $\\lambda$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_lam = 10\n",
    "print(\"[--] best lambda found: {}\".format(bestlam))\n",
    "w_best = ...\n",
    "Ypred_best = ...\n",
    "\n",
    "w_worst = ...\n",
    "Ypred_worst = ...\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "ax.scatter(Xtr, Ytr)\n",
    "ax.plot(Xtr, Ypred_best, '-',c=\"red\", label=\"with best $\\lambda$\")\n",
    "ax.plot(Xtr, Ypred_worst, '-',c=\"black\", label=\"with worst $\\lambda$\")\n",
    "ax.set_xlabel(\"$x$\")\n",
    "ax.set_ylabel(\"$y$\")\n",
    "ax.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the best $\\lambda$ found to train the model on the full training set and compute the test error on the following test set.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xte, Yte = linearRegrFunction(200, D, low_D, high_D, truew, sigma_noise)\n",
    "\n",
    "print(\"[--] best lambda found: {}\".format(bestlam))\n",
    "\n",
    "# Insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat the procedure on a different dataset (optional)\n",
    "\n",
    "Create new training **and** test datasets, sampled in a non-symmetric range (for example you can set the `low_D` and `high_D` parameters of the `linearRegrFunction` function to 2 and 5).\n",
    "\n",
    "Then repeat the K-fold CV procedure, and check whether the best regularization parameter changes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixGauss(means, sigmas, n):\n",
    "    \"\"\"\n",
    "    means : 2D array (num_classes, d)\n",
    "        Each row of the array gives the mean of the Gaussian in multiple dimensions for one class.\n",
    "        For binary classification problems, the number of rows should be 2!\n",
    "    sigmas : 1D array (num_classes)\n",
    "        The standard deviation for the Gaussian distribution of each class (isotropic Gaussian!)\n",
    "    \n",
    "    Example:\n",
    "    \n",
    "    >>> means = [[3, 0], [0, 0]]\n",
    "    >>> sigmas = [0.5, 1]\n",
    "    >>> X, Y = mixGauss(means, sigmas, n=100)\n",
    "    >>> fig, ax = plt.subplots()\n",
    "    >>> ax.scatter(X[Y == 1,0], X[Y == 1,1], marker='o', color='r')\n",
    "    >>> ax.scatter(X[Y == -1,0], X[Y == -1,1], marker='o', color='b')\n",
    "    \"\"\"\n",
    "    means = np.array(means)\n",
    "    sigmas = np.array(sigmas)\n",
    "\n",
    "    d = means.shape[1]\n",
    "    num_classes = sigmas.size\n",
    "    data = np.full((n * num_classes, d), np.inf)\n",
    "    labels = np.zeros(n * num_classes)\n",
    "\n",
    "    for idx, sigma in enumerate(sigmas):\n",
    "        data[idx * n:(idx + 1) * n] = np.random.multivariate_normal(\n",
    "            mean=means[idx], cov=np.eye(d) * sigmas[idx] ** 2, size=n)\n",
    "        labels[idx * n:(idx + 1) * n] = idx \n",
    "        \n",
    "    if(num_classes == 2):\n",
    "        labels[labels==0] = -1\n",
    "\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flipLabels(Y, perc):\n",
    "    if perc < 1 or perc > 100:\n",
    "        print(\"p should be a percentage value between 0 and 100.\")\n",
    "        return -1\n",
    "\n",
    "    if any(np.abs(Y) != 1):\n",
    "        print(\"The values of Ytr should be +1 or -1.\")\n",
    "        return -1\n",
    "\n",
    "    Y_noisy = np.copy(np.squeeze(Y))\n",
    "    if Y_noisy.ndim > 1:\n",
    "        print(\"Please supply a label array with only one dimension\")\n",
    "        return -1\n",
    "\n",
    "    n = Y_noisy.size\n",
    "    n_flips = int(np.floor(n * perc / 100))\n",
    "    idx_to_flip = np.random.choice(n, size=n_flips, replace=False)\n",
    "    Y_noisy[idx_to_flip] = -Y_noisy[idx_to_flip]\n",
    "\n",
    "    return Y_noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separatingFLR(Xtr, Ytr, Ypred):\n",
    "    fig, ax = plt.subplots()\n",
    "    xi = np.linspace(Xtr[:, 0].min(), Xtr[:, 0].max(), 200)\n",
    "    yi = np.linspace(Xtr[:, 1].min(), Xtr[:, 1].max(), 200)\n",
    "    X, Y = np.meshgrid(xi,yi)\n",
    "    \n",
    "    zi = griddata(Xtr, Ypred, (X,Y), method='linear')\n",
    "    \n",
    "    ax.contour(xi, yi, zi, 15, linewidths=2, colors='k', levels=[0])\n",
    "    # plot data points.\n",
    "    ax.scatter(Xtr[:,0], Xtr[:,1], c=Ytr, marker='o', s=100, zorder=10, alpha=0.8)\n",
    "    ax.xlim(Xtr[:,0].min(), Xtr[:,0].max())\n",
    "    ax.ylim(Xtr[:,1].min(), Xtr[:,1].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear logistic regression with gradient descent\n",
    "\n",
    "We define two functions:\n",
    " - the `optimal_gd_learning_rate(Xtr, reg_par)` function calculates the optimal learning rate for GD on a given dataset. You will need to use the SVD of the covariance matrix.\n",
    " - the `train_logreg_gd(Xtr, Ytr, reg_par, maxiter)` function estimates the classifier weights on the training set.\n",
    "\n",
    "<br>\n",
    "\n",
    "The parameters of the `optimal_gd_learning_rate` function are:\n",
    "- <b>Xtr</b> is the nxD matrix of training set inputs\n",
    "- <b>reg_par</b> is the value of the lammbda\n",
    "\n",
    "and it should output the learning rate $\\gamma$ (a scalar).\n",
    "You should use the `np.linalg.eigvalsh` function to calculate the eigenvalues of the covariance matrix.\n",
    "\n",
    "<br>\n",
    "\n",
    "The parameters of the `train_logreg_gd` function are:\n",
    "- <b>Xtr</b> is the nxD matrix of training set inputs\n",
    "- <b>Ytr</b> is the n vector of training set outputs\n",
    "- <b>reg_par</b> is the value of the lammbda\n",
    "- <b>maxiter</b> is the maximum number of iterations to run gradient descent\n",
    "\n",
    "and it should output:\n",
    "- <b>w</b> is the D vector of the estimated function parameters\n",
    "- <b>losses</b> is the vector of the loss at each iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_gd_learning_rate(Xtr, reg_par):\n",
    "    # Estimate the gamma parameter: the optimal learning rate for gradient descent\n",
    "    eigvals = ...\n",
    "    L = ...\n",
    "    gamma = 1 / L\n",
    "    return gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logreg_gd(Xtr, Ytr, reg_par, maxiter=100):\n",
    "    \"\"\"\n",
    "    Xtr : array of shape n, d\n",
    "    Ytr : array of shape n, or of shape n, 1\n",
    "    reg_par : regularization parameter (a scalar)\n",
    "    maxiter : the maximum number of gradient-descent iterations\n",
    "    \"\"\"\n",
    "    # Epsilon is a criterion for early stopping\n",
    "    epsilon = 1e-6\n",
    "    \n",
    "    # size of the input in the training\n",
    "    n, D = np.shape(Xtr)\n",
    "    \n",
    "    # initialization of the vector w\n",
    "    w = np.zeros((D, 1))\n",
    "    \n",
    "    # Set the learning rate optimally\n",
    "    gamma = optimal_gd_learning_rate(Xtr, reg_par)\n",
    "    \n",
    "    # initialization of some supporting variables\n",
    "    j=0\n",
    "    loss_old = 0\n",
    "    loss = float(\"inf\")\n",
    "    training_losses = np.zeros(maxiter + 1)\n",
    "    Ytr = Ytr.reshape(-1, 1)  # Convert from shape n, to shape n, 1\n",
    "    while j < maxiter and abs(loss - loss_old) >= epsilon:\n",
    "        loss_old = loss\n",
    "        j = j + 1\n",
    "        w = w - gamma * (...)\n",
    "        loss = ...\n",
    "\n",
    "        training_losses[j] = loss[0]\n",
    "    return w, training_losses[:j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation the function on the test set\n",
    "\n",
    "A function to perform predictions on a set of samples given the learned logistic regression weights\n",
    "\n",
    "##### Ypred, Ppred = predict_logreg(weights, Xte)\n",
    "where\n",
    "- <b>weights</b> is the D vector of the estimated function parameters\n",
    "- <b>X</b> is the matrix of input points of the training or test set.\n",
    "- <b>Ypred</b> is the vector of predictions\n",
    "- <b>Ppred</b> is the predicted probability of a point belonging to class +1. It will be 0 if the model is very confident the point belongs to class -1, it will be 1 if the model is very confident that the point belongs to class +1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_logreg(weights, X):\n",
    "    \"\"\"\n",
    "    weights : array of shape d, 1\n",
    "    X : array of shape n, d\n",
    "    \"\"\"\n",
    "    ypred = np.dot(X, weights)\n",
    "    # Try and understand what it does, deriving the formula\n",
    "    ppred = 1 / (1 + np.exp(-ypred))\n",
    "    # The outputs are reshaped to be 1D vectors.\n",
    "    return ypred.reshape(-1), ppred.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcError(Ypred, Y):\n",
    "    class_err = np.mean(np.sign(Ypred) != Y)\n",
    "    return class_err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Analysis\n",
    "\n",
    "Perform the following tasks for the initial analysis:\n",
    "\n",
    "1. Create two binary classification datasets (training and test sets) -- Use the same parameters. Visualize them in a scatter plot.\n",
    "\n",
    "2. Pick a reasonable value for lambda (e.g. reg_par = 0.1, 0.01, 0.001, ...) and train a logistic-regression model using the functions you have defined.\n",
    "\n",
    "3. Plot the loss at each iteration which is returned by the training function. The loss should decrease at each iteration, **if the loss does not decrease there is an error in the implementation!**\n",
    "\n",
    "4. Use the `separatingFLR` function to plot the separating curve obtained with the model\n",
    "\n",
    "5. Evaluate the error training and test sets.\n",
    "\n",
    "**Important #1**: The parameters used to generate the data are quite important. Try to make sure that the two classes are distinct (i.e. the means of the Gaussians should be different), but also not too far. Ideally a few points should overlap between the classes.\n",
    "Make sure to generate at least 100 points for both train and test sets.\n",
    "\n",
    "**Important #2**: since we are implementing a linear model, we must add a bias term -- otherwise the weights will draw a line which always goes through 0. An alternative to adding a bias term to the model is to add a feature comprising all ones to the data-column. We can use the following code for this:\n",
    "```\n",
    "Xtr_wbias = np.hstack((Xtr, np.ones((Xtr.shape[0], 1))))\n",
    "Xts_wbias = np.hstack((Xts, np.ones((Xts.shape[0], 1))))\n",
    "```\n",
    "what whill be the resulting shape of the weight vector `w`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Help for data-generation\n",
    "means = ...\n",
    "sigmas = ...\n",
    "\n",
    "Xtr, Ytr = mixGauss(means, sigmas, 300)\n",
    "Xts, Yts = mixGauss(means, sigmas, 100)\n",
    "\n",
    "Xtr_wbias = ...\n",
    "Xts_wbias = ...\n",
    "\n",
    "# TODO: Plot training and test sets, coloring the two classes differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estw, losses = train_logreg_gd(...)  # Careful: use the datasets with the bias.\n",
    "# TODO: Plot the losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot the separating curve (on the test set)\n",
    "yts_pred, ts_score = ...\n",
    "separatingFLR(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate the errors\n",
    "tr_pred, _ = predict_logreg(...)\n",
    "tr_err = calcError(...)\n",
    "ts_pred, _ = predict_logreg(...)\n",
    "ts_err = calcError(...)\n",
    "print(\"Training error: %.2f%%, Test error: %.2f%%\" % (tr_err * 100, ts_err * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the confidence of logistic regression predictions\n",
    "\n",
    "Logistic regression does not only output *pointwise predictions* (the class to which a point belongs), but it also gives the **probability** that a test point belongs to a certain class.\n",
    "\n",
    "This probability can be very useful to interpret the outputs of your model: in certain cases it might be better to **not predict anything** if the confidence of the model is low (i.e. if the model predicts a probability of 0.5 in a binary setting, the model is not sure which class a point belongs to).\n",
    "\n",
    "In this part of the lab, we will\n",
    " 1. Implement a function which allows to visualize the confidence of predictions (`plot_logreg_confidence`)\n",
    " 2. Train a logistic regression model, and use the visualization function to see where the low confidence region of prediction lies.\n",
    " 3. Calculate the error on **just the high-confidence** predictions, and see how the it changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_logreg_confidence(X, Y, pred_confidence, threshold):\n",
    "    \"\"\"\n",
    "    X : a (n, d) dataset\n",
    "    Y : a (n, ) array of targets\n",
    "    pred_confidence : A (n, ) array of probabilities predicted from X\n",
    "    threshold : a float between 0 and 0.5 determining the probability threshold we use to \n",
    "                consider neutral predictions. For example if threshold=0.1 then all probabilities\n",
    "                between 0.4 and 0.6 will be considered neutral (that is, neither belonging to class +1\n",
    "                or to class -1).\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    xi = np.linspace(X[:, 0].min(), X[:, 0].max(), 200)\n",
    "    yi = np.linspace(X[:, 1].min(), X[:, 1].max(), 200)\n",
    "    X_grid, Y_grid = np.meshgrid(xi,yi)\n",
    "    \n",
    "    zi = griddata(X, pred_confidence, (X_grid, Y_grid), method='linear')\n",
    "    \n",
    "    ax.contour(xi, yi, zi, linewidths=2, levels=[0.5 - threshold, 0.5 + threshold])\n",
    "    \n",
    "    # Plot data points\n",
    "    ax.scatter(X[:,0], X[:,1], c=Y, \n",
    "               marker='o', s=100, zorder=10, alpha=0.8)\n",
    "    ax.set_xlim(X[:,0].min(), X[:,0].max())\n",
    "    ax.set_ylim(X[:,1].min(), X[:,1].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some data\n",
    "Xtr, Ytr = mixGauss([[0,1],[1,1]], [0.4,0.4], 100)\n",
    "Xts, Yts = mixGauss([[0,1],[1,1]], [0.4,0.4], 100)\n",
    "\n",
    "Xtr_wbias = np.hstack((Xtr, np.ones((Xtr.shape[0], 1))))\n",
    "Xts_wbias = np.hstack((Xts, np.ones((Xts.shape[0], 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train a logistic regression model, and obtain the probability predictions for the test set\n",
    "...\n",
    "ts_pred, ts_conf = predict_logreg(...)\n",
    "# TODO: Plot the logistic regression confidence for different thresholds. Interpret what you see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_error_with_confidence(prob_pred, Y, threshold):\n",
    "    \"\"\"\n",
    "    Calculate the classification error on only the predictions with a high confidence!\n",
    "    \"\"\"\n",
    "    class_m1 = prob_pred <= 0.5 - threshold\n",
    "    class_p1 = prob_pred > 0.5 + threshold\n",
    "    \n",
    "    err_class_m1 = Y[class_m1] != -1\n",
    "    err_class_p1 = Y[class_p1] != 1\n",
    "    return (np.sum(err_class_m1) + np.sum(err_class_p1)) / (len(err_class_m1) + len(err_class_p1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train a log-reg model, and calculate the error on only the high-confidence examples from the test\n",
    "#       set. How do you expect this error to behave as you change the threshold?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "a472841e84b21350b5602f95a88580ab329d461f508f41063227ebad822f9379"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
