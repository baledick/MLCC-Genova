{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e10397a",
   "metadata": {},
   "source": [
    "# Lab 1 - K-Nearest Neighbours\n",
    "\n",
    "This lab is about the implementation and analysis of the KNN algorithm for classification problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efd1357a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages used\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from scipy.interpolate import griddata\n",
    "import os\n",
    "import pickle\n",
    "np.random.seed(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816ed234",
   "metadata": {},
   "source": [
    "### Data generation for binary classification\n",
    "\n",
    "We start generating a training set for binary classification problems. Consider the following function, that generates random 2D points on the plane and assigns them a binary label according to their position w.r.t. a linear separator.\n",
    "\n",
    "Implement a function `linearBinaryClass` which given a sample size `n`, lower and upper bounds `low_D, high_D` for the domain of the samples and the linear function parameters `m, q`, generates a binary classification dataset i.e. it returns `X` and `Y`.\n",
    "\n",
    "The signature of the function is the following:\n",
    "\n",
    "`X, Y = linearBinaryClass(n, low_D, high_D, m, q)`\n",
    "\n",
    "where\n",
    "- **n** is the number of samples to be generated\n",
    "- **low_D** and **high_D** are, respectively, the lower and upper bounds for the domain of the samples i.e.\n",
    "    $$\n",
    "    X_{i,j} \\sim \\mathcal{U}(\\text{low\\_D}, \\text{high\\_D})\n",
    "    $$\n",
    "- **m, q** are the linear function parameters\n",
    "- **X**, **Y**: 2-dimensional samples (X) associated with 1-dimensional binary labels (Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21793caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearBinaryClass(n, low_D, high_D, m, q):\n",
    "    X = (np.random.rand(n, 2) * (high_D - low_D)) + low_D\n",
    "    Y = np.sign((X[:,1] - (m * X[:,0]) + q))\n",
    "    \n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddedf9d",
   "metadata": {},
   "source": [
    "### Computing the distance between input points\n",
    "\n",
    "In order to build the KNN estimator we need to resort to a distance function.\n",
    "\n",
    "Consider a function that computes the euclidean distance between two points..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab66b6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidDistance(P1,P2):\n",
    "    return np.linalg.norm(P1-P2,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835e1b89",
   "metadata": {},
   "source": [
    "... and then a function that computes all the distance between two set of points stored in two matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a29b3fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def allDistances(X1, X2):\n",
    "    D = np.zeros((X1.shape[0], X2.shape[0]))\n",
    "    for idx1 in range(len(X1)):\n",
    "        for idx2 in range(len(X2)):\n",
    "            D[idx1,idx2] = euclidDistance(X1[idx1,:],X2[idx2,:])\n",
    "    return D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e143880",
   "metadata": {},
   "source": [
    "The usage is the following:\n",
    "\n",
    "**D = allDistances(X1,X2)**\n",
    "\n",
    "where\n",
    "- **X1** is a matrix of size $n_1 \\times D$, where each row is a D-dimensional point\n",
    "- **X2** is a matrix of size $n_2 \\times D$, where each row is a D-dimensional point\n",
    "- **D** is a matrix of size $n_1 \\times n_2$, where each element `D[i,j]` is the distance between points (`X1[i, :]`, `X2[j, :]`)\n",
    "\n",
    "<font color=red>**Optional**</font>:\n",
    "The `allDistances` function is very inefficient. Can you come up with a faster way to compute the distances between all pairs of points in two matrices?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90bfcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide here a faster version of the function allDistances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d86b433",
   "metadata": {},
   "source": [
    "### Adding noise to the samples\n",
    "\n",
    "To make the task harder, we may want to perturb the labels with some noise.\n",
    "\n",
    "In our case, we have binary labels and a common way of adding noise is to flip the value of a small percentage of the labels. For example, if a label was $+1$ it will become $-1$.\n",
    "\n",
    "Implement the `flipLabels` function which takes two arguments:\n",
    " - `Y`, the numpy array of original labels\n",
    " - `P`, an integer between 1 and 100 specifying the percentage of labels which will be flipped\n",
    "and returns an array of the same shape as `Y`, which contains the noisy labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732662c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flipLabels(Y, P):\n",
    "    if P < 1 or P > 100:\n",
    "        raise Exception('P should be between 1 and 100')\n",
    "        \n",
    "    indices_to_flip = np.random.choice(range(len(Y)), int(len(Y) * (P / 100)), replace=False)\n",
    "    Y_noisy = Y.copy()\n",
    "    Y_noisy[indices_to_flip] *= -1\n",
    "    \n",
    "    return Y_noisy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea30797",
   "metadata": {},
   "source": [
    "### The KNN classifier\n",
    "\n",
    "We are now ready to use the KNN algorithm to estimate the classification function. \n",
    "\n",
    "Implement the `kNNClassify` function which train a KNN classifier on given training set and returns predictions on a given test set. \n",
    "\n",
    "The signature of the function is the following:\n",
    "\n",
    "`Ypred = kNNClassify(Xtr, Ytr, k, Xte)`\n",
    "\n",
    "where\n",
    "- **Xtr** is a matrix of size [ntr, D], where each row is a D-dimensional point (INPUT IN THE **TRAINING SET**)\n",
    "- **Ytr** is an array of size [ntr], where each element is a binary label (OUTPUT IN THE **TRAINING SET**)\n",
    "- **k** is the number of neighbours to be considered\n",
    "- **Xte** is a matrix of size [nte, D], where each row is a D-dimensional point (INPUT IN THE **TEST SET**)\n",
    "- **Ypred** is an array of size [nte], where each element is a binary label (ESTIMATED OUTPUT FOR THE **TEST SET**)\n",
    "\n",
    "where [ntr] and [nte] are respectively the number of points in the training set and the number of points in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd91f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kNNClassify(Xtr, Ytr, k, Xte):\n",
    "\n",
    "    n_train = Xtr.shape[0] # number of the training inputs\n",
    "    n_test = Xte.shape[0] # number of the test inputs\n",
    "\n",
    "    if any(np.abs(Ytr) != 1):\n",
    "        raise Exception(\"The values of Ytr should be +1 or -1.\")\n",
    "\n",
    "    if k > n_train:\n",
    "        print(\"k is greater than the number of points, setting k=n_train\")\n",
    "        k = n_train\n",
    "\n",
    "    Ypred = np.zeros(n_test)\n",
    "\n",
    "    \n",
    "    # Compute all the distances from test input and training input\n",
    "    dist = ... \n",
    "    \n",
    "    # For each test point, the predicted class will be \n",
    "    # the sign of the average label of the k nearest points\n",
    "    for idx in range(n_test):\n",
    "        # Insert your code here\n",
    "        \n",
    "    return Ypred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218e8e32",
   "metadata": {},
   "source": [
    "### Visualizing the separating function\n",
    "\n",
    "The visualization of the separating function on the training set, i.e. the function estimated by classification algorithm for discriminating between classes, is of benefit for appreciating the behavior of the binary classifier. To visualize the separating function use the following:\n",
    "\n",
    "**_separatingFkNN(Xtr, Ytr, k)_**\n",
    "\n",
    "where\n",
    "- **Xtr** is a matrix of size [ntr, D], where each row is a D-dimensional point (INPUT IN THE **TRAINING SET**)\n",
    "- **Ytr** is an array of size [ntr], where each element is a binary label (OUTPUT IN THE **TRAINING SET**)\n",
    "- **k** is the number of neighbours to be considered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff7531e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separatingFkNN(Xtr, Ytr, k):\n",
    "    Ypred = kNNClassify(Xtr=Xtr, Ytr=Ytr, k=k, Xte=Xtr)\n",
    "\n",
    "    x = Xtr[:, 0]\n",
    "    y = Xtr[:, 1]\n",
    "    xi = np.linspace(x.min(), x.max(), 200)\n",
    "    yi = np.linspace(y.min(), y.max(), 200)\n",
    "    zi = griddata((x, y), Ypred, (xi[None, :], yi[:, None]), method='linear')\n",
    "\n",
    "    plt.subplots()\n",
    "    CS = plt.contour(xi, yi, zi, 15, linewidths=2, colors='k', levels=[0])\n",
    "    # plot data points.\n",
    "    plt.scatter(x, y, c=Ytr, marker='o', s=50, zorder=10, alpha=0.8)\n",
    "    plt.xlim(x.min(), x.max())\n",
    "    plt.ylim(x.min(), x.max())\n",
    "    msg = 'Separating function, k='+str(k);\n",
    "    plt.title(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7095697b",
   "metadata": {},
   "source": [
    "### Evaluating the goodness of a classifier\n",
    "\n",
    "To evaluate how good is the classification function estimated by the KNN, we compare the predicted binary labels and expected (true) ones, with the following function:\n",
    "\n",
    "`err = calcError(Ypred, Ytrue)`\n",
    "\n",
    "where\n",
    "- **Ypred** is an array of size $n$, where each element is a binary label predicted by the classifier\n",
    "- **Ytrue** is an array of size $n$, where each element is the true binary label\n",
    "- **err** is the fraction of wrongly classified elements wrt the total number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485a06ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcError(Ypred, Ytrue):\n",
    "    return (np.count_nonzero(Ypred != Ytrue)) / len(Ytrue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43015218",
   "metadata": {},
   "source": [
    "### Noise-free case\n",
    "\n",
    "In this assignment, we start to play with the NN classifier. In specific:\n",
    "\n",
    "- 1.Generate a training set WITHOUT NOISE\n",
    "- 2.Visualize the separating curve for the **NN** classifier \n",
    "- 3.Generate a test set with the same amount of noise as the training set.\n",
    "- 4.Compute the error on the TRAINING and then on the TEST SET\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcb912a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate training and test sets, build and evaluate the KNN classifier\n",
    "\n",
    "n = 200\n",
    "\n",
    "m = 0.9\n",
    "q = 0.0\n",
    "\n",
    "low_D = -10\n",
    "high_D = 10\n",
    "\n",
    "k = 1\n",
    "\n",
    "# Generate a training set WITHOUT NOISE\n",
    "Xtr, Ytr = ... \n",
    "\n",
    "# Visualize the separating curve for the NN classifier \n",
    "\n",
    "# Generate a test set WITHOUT NOISE\n",
    "Xte, Yte = ...\n",
    "\n",
    "# Evaluate the NN classifier on the TEST SET\n",
    "Ypred = ... \n",
    "\n",
    "# Compute the error on the TEST SET\n",
    "err = ...\n",
    "\n",
    "print(\"With K=%d the error on the test set is \" % (k), err)\n",
    "\n",
    "# How the classifier perform on the TRAINING SET instead?\n",
    "Ypredtr = ... \n",
    "errtr = ...\n",
    "print(\"With K=%d the error on the training set is \" % (k), errtr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dd5634",
   "metadata": {},
   "source": [
    "### Adding noise\n",
    "\n",
    "Keeping the parameters of the function and the number of points as in the previous assignment:\n",
    "\n",
    "- 1.Generate a training set WITH NOISE (for instance with 10% of flipped labels)\n",
    "- 2.Visualize the separating curve for the **NN** classifier \n",
    "- 3.Generate a test set with the same amount of noise as the training set.\n",
    "- 4.Evaluate the NN classifier on the TRAINING and on the TEST SET\n",
    "- 5.Compute the obtained error on the TRAINING and on the TEST SET\n",
    "\n",
    "- Repeat the steps from 1 to 5 with the **KNN** algorithm, setting for instance K=5\n",
    "\n",
    "OBSERVE WHAT CHANGES..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4396c0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... your code goes here... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c040579",
   "metadata": {},
   "source": [
    "Describe here what changes: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba94bb4",
   "metadata": {},
   "source": [
    "### Some scenarios\n",
    "\n",
    "Analyse the performance of the **KNN** with plots, considering in particular the following:\n",
    "\n",
    "\n",
    "\n",
    "- **SCENARIO 1**: Fix the number n of points to 200, fix the amount of noise to 10%, and plot the performance of the KNN classifier on TRAINING and TEST SETS as you increase the value of K\n",
    "\n",
    "- **SCENARIO 2**: Fix the number n of points to 200, fix the value of K to a reasonable number of neighbours, and plot the performance of the KNN classifier on TRAINING and TEST SETS as you increase the amount of noise\n",
    "\n",
    "- **SCENARIO 3**: Fix noise and K to two reasonable values, fix the number of TEST samples to 300, and plot the performance of the KNN classifier on TRAINING and TEST SETS as you increase the number of TRAINING samples (e.g. from 30 to 300 with steps 30)\n",
    "\n",
    "- **SCENARIO 4**: Fix K and plot the performance of the KNN classifier on TRAINING and TEST SETS as you increase the amount of noise and the number of points\n",
    "\n",
    "For each scenario, describe what happen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9317dacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... your code for SCENARIO 1 goes here...    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7345c84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... your code for SCENARIO 2 goes here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6b333b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... your code for SCENARIO 3 goes here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8018a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... your code for SCENARIO 4 goes here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0018ca7a",
   "metadata": {},
   "source": [
    "Insert here your observations:\n",
    "- **SCENARIO 1**:\n",
    "- **SCENARIO 2**:\n",
    "- **SCENARIO 3**:\n",
    "- **SCENARIO 4**:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "a472841e84b21350b5602f95a88580ab329d461f508f41063227ebad822f9379"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
